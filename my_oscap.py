import argparse
import os
from os import error
from subprocess import CalledProcessError
import sys
import subprocess
import json
import socket

from time import gmtime, strftime
from bs4 import BeautifulSoup

class Report():

    result_folder = '/root/scans/'

    def parseResult(self, rule_result_tag):
        '''
        Returns a dict from the XML tags. Tags saved are severity and result text.

            Parameters: 
                rule_result_tag: Rule Result Tag obtained from result XML
        '''
        result = {}
        result['severity'] = rule_result_tag.get('severity')
        result['result'] = rule_result_tag.result.text
        return result

    def scanReport(self, xmlFile):
        '''
        Scans the report generated by oscap, focusing on the TestResult tag to obtain the results of the scan.
        Generates a JSON file located at result_folder containing the executed rules, text description and pass or fail status.
        Prints a summary of the results to stdout.

            Parameters: 
                xmlFile: String containing the path to the result XML file generated by oscap.
        '''
        rule_result = {'stats':{'total':0, 'pass':0, 'fail':0}}

        try:
            with open(xmlFile, 'r') as file:
                soup = BeautifulSoup(file, 'xml')
        except IOError as err:
            print(err)
            raise err

        test_target = soup.find('TestResult').target.text
        test_time = soup.find('TestResult').get('end-time')

        for rule_result_tag in soup.find_all('rule-result'):
            result = rule_result_tag.result.text
            if(result == 'notselected' or result == 'notapplicable'):
                continue
            rule_result[rule_result_tag.get('idref')] = self.parseResult(rule_result_tag)
            if result == 'pass':
                rule_result['stats']['pass']+=1
            elif result == 'fail':
                rule_result['stats']['fail']+=1

        rule_result['stats']['total'] = len(rule_result) - 1
        
        try:
            with open(self.result_folder+test_target+'_'+test_time+'.json', 'w') as file:
                json.dump(rule_result, file)
        except IOError as err:
            print('Could not write to output dir at {}'.format(self.result_folder))
            raise err

        for key in rule_result:
            if key is not 'stats':
                print('ID: {}\nSeverity: {}\nResult: {}\n'.format(key, rule_result[key]['severity'], rule_result[key]['result']))
        
        print('Test Target: {}\nUTC Time: {}\nTotal tests: {}\nPassed: {}\nFailed: {}\nNot Executed: {}'.format(test_target, test_time, rule_result['stats']['total'], rule_result['stats']['pass'], rule_result['stats']['fail'], rule_result['stats']['total'] - rule_result['stats']['pass'] - rule_result['stats']['fail']))

    def getFilenameById(self, fileID):
        '''
        Returns a file that matches with the ID provided as parameter. 

            Parameters: 
                fileID: ID of the file to return
        '''
        try:
            files = os.listdir(self.result_folder)
        except IOError as err:
            print('Output dir expected at {} was not found'.format(self.result_folder))
            raise err
            
        id = 1

        sortedFiles = sorted(files, key=lambda x: os.path.splitext(x.split('_')[-1])[0])

        for file in sortedFiles:
            if(int(fileID) == id):
                return file
            id+=1
        
        print('File with ID {} was not found'.format(fileID))
        raise FileNotFoundError()

    def compareReports(self, id_1, id_2):
        '''
        Compares two reports generated by this tool. Returns the summary statistics of each report, along with 4 lists:
            Issues resolved in the second report.
            New issues found in the second report.
            Executed rules from the first report missing in the second report.
            Executed rules from the second report missing in the first report.

            Parameters: 
                id_1: ID of the file to compare
                id_2: ID of the file to compare against
        '''
        fixed_issues = []
        new_issues = []
        unmatched_issues_1 = []
        unmatched_issues_2 = []
        try:
            file_1 = self.getFilenameById(id_1)
            file_2 = self.getFilenameById(id_2)
        except IOError as err:
            raise err

        with open(self.result_folder + file_1, 'r') as file:
            rule_result_1 = json.load(file)

        with open(self.result_folder + file_2, 'r') as file:
            rule_result_2 = json.load(file)

        for key in rule_result_1:
            if key != 'stats':
                key_2 = rule_result_2.get(key, None)
                if key_2 is None:
                    unmatched_issues_1.append(key)

        for key in rule_result_2:
            if key != 'stats':
                key_1 = rule_result_1.get(key, None)
                if key_1 is not None:
                    if rule_result_1[key]['result'] == 'pass' and rule_result_2[key]['result'] == 'fail':
                        new_issues.append(key)
                    elif rule_result_1[key]['result'] == 'fail' and rule_result_2[key]['result'] == 'pass':
                        fixed_issues.append(key)
                else :
                    unmatched_issues_2.append(key)

        print('First scan summary statistics\nHost: {}\nDate: {}\nTotal: {}\nPassed: {}\nFailed: {}\nNotExecuted: {}\n'.format(file_1[:-25], file_1[-24:-5], rule_result_1['stats']['total'], rule_result_1['stats']['pass'], rule_result_1['stats']['fail'], rule_result_1['stats']['total'] - rule_result_1['stats']['pass'] - rule_result_1['stats']['fail']))
        print('Second scan summary statistics\nHost: {}\nDate: {}\nTotal: {}\nPassed: {}\nFailed: {}\nNot Executed: {}\n'.format(file_2[:-25], file_2[-24:-5], rule_result_2['stats']['total'], rule_result_2['stats']['pass'], rule_result_2['stats']['fail'], rule_result_2['stats']['total'] - rule_result_2['stats']['pass'] - rule_result_2['stats']['fail']))
        
        if fixed_issues:
            print('Fixes in second scan')
            for id in fixed_issues:
                print('ID: {}\tSeverity: {}\n'.format(id, rule_result_2[id]['severity']))

        if new_issues:
            print('New issues in second scan')
            for id in new_issues:
                print('ID: {}\tSeverity: {}\n'.format(id, rule_result_2[id]['severity']))

        if unmatched_issues_1:
            print('Rules missing from second scan')
            for id in unmatched_issues_1:
                print('ID: {}\tSeverity: {}\tResult: {}\n'.format(id, rule_result_1[id]['severity'], rule_result_1[id]['result']))

        if unmatched_issues_2:
            print('New rules executed in second scan')
            for id in unmatched_issues_2:
                print('ID: {}\tSeverity: {}\tResult: {}\n'.format(id, rule_result_2[id]['severity'], rule_result_2[id]['result']))

        if not fixed_issues and not new_issues and not unmatched_issues_1 and not unmatched_issues_2:
            print('No changes were detected')

    def showScanHistory(self):
        '''
        Displays a list of json files containing past reports generated, along with an ID that can be provided to the tool for comparison or to display a specific report. 
        Files are sorted by date generated.
        '''
        try:
            files = os.listdir(self.result_folder)
        except IOError as err:
            print('Output dir expected at {} was not found'.format(self.result_folder))
            return

        id = 1

        sortedFiles = sorted(files, key=lambda x: os.path.splitext(x.split('_')[-1])[0])

        print("Scan history: ")
        for file in sortedFiles:
            print('ID: {} Filename: {}'.format(id, file))
            id+=1
        if id == 1:
            print('No scans found')

    def getReportById(self, fileID):
        '''
        Displya summary report with ID matching parameter.
        The information displayed is: 
            All rules, with ID, severity and result(pass or fail)
            Summary statistics:
                Target Hostname
                UTC Date of scan
                Number of rules checked
                Number of rules passed
                Number of rules failed
                Number of rules not executed

            Parameters:
                fileID: ID of the file to display
        '''
        try:
            filename = self.getFilenameById(fileID)
        except IOError:
            return

        try:
            with open(self.result_folder + filename, 'r') as file:
                rule_result = json.load(file)
        except IOError as err:
            print('File {} not found'.format(err.filename))
            return

        for key in rule_result:
            if key != 'stats':
                print('ID: {}\nSeverity: {}\nResult: {}\n'.format(key, rule_result[key]['severity'], rule_result[key]['result']))
        
        print('Test Target: {}\nUTC Time: {}\nTotal rules: {}\nPassed: {}\nFailed: {}\nNot Executed: {}'.format(filename[:-25], filename[-24:-5], rule_result['stats']['total'], rule_result['stats']['pass'], rule_result['stats']['fail'], rule_result['stats']['total'] - rule_result['stats']['pass'] - rule_result['stats']['fail']))

def parseCommandLine():
        '''
        Parses the command line options
        '''
        parser = argparse.ArgumentParser(description='Command line tool to execute OpenSCAP scans of the Oracle Linux 7 system using the "stig" profile from the "scap-security-guide" package')
        group = parser.add_mutually_exclusive_group()
        group.add_argument('--scan', action='store_true', help='start a new scan')
        group.add_argument('--history', action='store_true', help='list past scans')
        group.add_argument('--scan-id', help='show results of scan with a certain id')
        group.add_argument('--compare', nargs=2, help='compare 2 scans by id')

        if(len(sys.argv) == 1):
            parser.print_help(sys.stderr)
            sys.exit(1)
        return parser.parse_args()


def main():
    reportInstance = Report()
    args = parseCommandLine()

    if args.scan:
        hostname = socket.gethostname()
        date = strftime("%Y-%m-%dT%H:%M:%S", gmtime())

        try :
            subprocess.check_call([
                    'oscap',
                    'xccdf', 
                    'eval', 
                    '--profile', 
                    'stig', 
                    '--results', 
                    '/root/openscap/results/'+hostname+'_'+date+'.xml',
                    '--cpe',
                    '/usr/share/xml/scap/ssg/content/ssg-ol7-cpe-dictionary.xml',
                    '/usr/share/xml/scap/ssg/content/ssg-ol7-xccdf.xml',
                ])
        except CalledProcessError as cpe:
            if cpe.returncode != 2:
                print('oscap call failed with error code {}'.format(cpe.returncode))
                exit(cpe.returncode)

        try:
            reportInstance.scanReport('/root/openscap/results/'+hostname+'_'+date+'.xml')
        except:
            exit(1)

    if args.scan_id:
        try:
            reportInstance.getReportById(args.scan_id)
        except:
            exit(1)

    if args.compare:
        try:
            reportInstance.compareReports(args.compare[0], args.compare[1])
        except:
            exit(1)
        
    if args.history:
        try:
            reportInstance.showScanHistory()
        except:
            exit(1)

if __name__ == "__main__":
    main()